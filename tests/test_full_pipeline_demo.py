"""
Full Pipeline Demo for Hebrew Phoneme Prediction Model

This demo shows the complete end-to-end pipeline for fine-tuning DictaBERT
to predict IPA phonemes for Hebrew text using a multi-head classification approach.
"""
import pytest
from src.tokenize import (
    encode, 
    decode, 
    Prediction, 
    reconstruct_sentence,
    CONSONANT_CLASSES,
    VOWEL_CLASSES
)
from src.preprocess import normalize_ipa, normalize_hebrew


class TestFullPipelineDemo:
    """
    Demonstrates the complete pipeline from raw text to phoneme predictions.
    
    Architecture Overview:
    - Base Model: DictaBERT (character-level BERT pretrained on Hebrew)
    - Task: Multi-head classification (4 heads per character)
    - Heads: consonant (24 classes), vowel (6 classes), stress (binary), flip_vowel (binary)
    - Training: Character-level labels, sentence-level context via BERT attention
    """
    
    def test_pipeline_step_by_step(self):
        """
        Complete pipeline walkthrough: text â†’ encoding â†’ training labels â†’ 
        predictions â†’ decoded IPA â†’ final sentence
        """
        print("\n" + "="*70)
        print("FULL PIPELINE DEMO: Hebrew Phoneme Prediction")
        print("="*70)
        
        # Input: Hebrew sentence
        text = '×©×œ×•× ×¢×•×œ×'
        print(f"\nğŸ“ Input Text: {text}")
        print(f"   Characters: {[c for c in text]}")
        
        # Step 1: Prepare training data (aligned IPA per character)
        ipa_aligned = normalize_ipa('Êƒa lËˆo Ã˜ m Ã˜ Ê”o lËˆa Ã˜ m')
        print(f"\nğŸ¯ Training Target (aligned IPA per char):")
        print(f"   {ipa_aligned}")
        print(f"   Split: {ipa_aligned.split(' ')}")
        
        # Step 2: Encode to multi-head labels
        encoded = encode(text, ipa_aligned)
        print(f"\nğŸ”¢ Encoded Labels (multi-head):")
        print(f"   Consonants: {encoded.consonant}")
        print(f"   Vowels:     {encoded.vowel}")
        print(f"   Stress:     {encoded.stress}")
        print(f"   Flip:       {encoded.flip_vowel}")
        print(f"\n   Class mappings:")
        print(f"   - Consonants: {CONSONANT_CLASSES}")
        print(f"   - Vowels: {VOWEL_CLASSES}")
        
        # Step 3: Simulate predictions (in real model, these come from forward pass)
        predictions = Prediction(
            consonant=encoded.consonant,
            vowel=encoded.vowel,
            stress=encoded.stress,
            flip_vowel=encoded.flip_vowel
        )
        print(f"\nğŸ¤– Model Predictions (simulated as perfect here):")
        print(f"   Shape: {len(predictions.consonant)} characters")
        
        # Step 4: Decode predictions back to per-character IPA
        ipa_decoded = decode(text, predictions, preserve_unknown=True)
        print(f"\nğŸ“¤ Decoded IPA (per-character):")
        print(f"   {ipa_decoded}")
        
        # Step 5: Reconstruct final sentence
        final_sentence = reconstruct_sentence(text, ipa_decoded)
        print(f"\nâœ¨ Final Output (natural sentence):")
        print(f"   {final_sentence}")
        print(f"\n   Explanation: Phonemes joined per word, spaces preserved")
        
        print("\n" + "="*70)
        
        # Verify correctness
        # Note: decoded has double space for space character
        assert ipa_decoded == 'Êƒa lËˆo Ã˜ m   Ê”o lËˆa Ã˜ m'
        assert final_sentence == 'ÊƒalËˆom Ê”olËˆam'
    
    def test_mixed_content_pipeline(self):
        """
        Demonstrate handling of mixed Hebrew-English content.
        Shows how non-Hebrew characters are handled differently.
        """
        print("\n" + "="*70)
        print("MIXED CONTENT DEMO: Hebrew + English")
        print("="*70)
        
        text = 'hello ×©×œ×•× world'
        print(f"\nğŸ“ Input: {text}")
        
        # Training format: non-Hebrew gets Ã˜ labels
        ipa_training = normalize_ipa('Ã˜ Ã˜ Ã˜ Ã˜ Ã˜ Ã˜ Êƒa lËˆo Ã˜ m Ã˜ Ã˜ Ã˜ Ã˜ Ã˜ Ã˜')
        print(f"\nğŸ¯ Training IPA (Ã˜ for non-Hebrew): {ipa_training}")
        
        # Encode
        encoded = encode(text, ipa_training)
        print(f"\nğŸ”¢ Encoded Labels:")
        print(f"   Consonants: {encoded.consonant}")
        print(f"   Note: -100 values are ignored in loss (non-Hebrew chars)")
        print(f"   -100 positions: {[i for i, c in enumerate(encoded.consonant) if c == -100]}")
        
        # Decode with preserve_unknown=True
        predictions = Prediction(
            consonant=encoded.consonant,
            vowel=encoded.vowel,
            stress=encoded.stress,
            flip_vowel=encoded.flip_vowel
        )
        ipa_decoded = decode(text, predictions, preserve_unknown=True)
        print(f"\nğŸ“¤ Decoded (preserve_unknown=True):")
        print(f"   {ipa_decoded}")
        print(f"   English chars preserved as-is")
        
        # Reconstruct
        final = reconstruct_sentence(text, ipa_decoded)
        print(f"\nâœ¨ Final: {final}")
        print(f"   Hebrew: ÊƒalËˆom (phonemes joined)")
        print(f"   English: hello, world (preserved)")
        
        print("\n" + "="*70)
        
        assert final == 'hello ÊƒalËˆom world'
    
    def test_context_aware_learning(self):
        """
        Demonstrate why we use BERT (context-aware) instead of character-only model.
        Same letter can have different phonemes based on context.
        """
        print("\n" + "="*70)
        print("CONTEXT-AWARE LEARNING DEMO")
        print("="*70)
        
        # Example: ×• can be 'v' or 'o' depending on context
        test_cases = [
            ('×•', 'v', "consonant usage"),
            ('×©×œ×•×', 'Êƒa lËˆo Ã˜ m', "×• as vowel 'o' in middle"),
        ]
        
        print("\nğŸ’¡ Why BERT Context Matters:")
        print("   Same Hebrew letter can have different pronunciations")
        print("   depending on surrounding characters.\n")
        
        for text, ipa_target, description in test_cases:
            ipa_norm = normalize_ipa(ipa_target)
            print(f"   Text: {text:10s} â†’ IPA: {ipa_norm:20s} ({description})")
        
        print("\n   BERT's attention mechanism provides context from:")
        print("   - Previous characters")
        print("   - Following characters")
        print("   - Word boundaries")
        print("   - This enables accurate phoneme prediction!")
        
        print("\n" + "="*70)
    
    def test_training_vs_inference_modes(self):
        """
        Show the difference between training format and inference output.
        """
        print("\n" + "="*70)
        print("TRAINING vs INFERENCE MODES")
        print("="*70)
        
        text = 'App ×©×œ×•× 2024'
        ipa_training = normalize_ipa('Ã˜ Ã˜ Ã˜ Ã˜ Êƒa lËˆo Ã˜ m Ã˜ Ã˜ Ã˜ Ã˜ Ã˜')
        
        print(f"\nğŸ“ Input: {text}")
        
        # Training mode: preserve_unknown=False
        encoded = encode(text, ipa_training)
        preds = Prediction(
            consonant=encoded.consonant,
            vowel=encoded.vowel,
            stress=encoded.stress,
            flip_vowel=encoded.flip_vowel
        )
        
        training_output = decode(text, preds, preserve_unknown=False)
        print(f"\nğŸ“š Training Format (preserve_unknown=False):")
        print(f"   {training_output}")
        print(f"   All non-Hebrew â†’ Ã˜ (consistent labels)")
        
        # Inference mode: preserve_unknown=True
        inference_output = decode(text, preds, preserve_unknown=True)
        print(f"\nğŸš€ Inference Format (preserve_unknown=True):")
        print(f"   {inference_output}")
        print(f"   Non-Hebrew preserved as-is")
        
        # Final reconstruction
        final = reconstruct_sentence(text, inference_output)
        print(f"\nâœ¨ User-Facing Output:")
        print(f"   {final}")
        print(f"   Natural sentence with phonemes")
        
        print("\n" + "="*70)
        
        assert final == 'App ÊƒalËˆom 2024'
    
    def test_model_architecture_summary(self):
        """
        Display the model architecture and training details.
        """
        print("\n" + "="*70)
        print("MODEL ARCHITECTURE SUMMARY")
        print("="*70)
        
        print("\nğŸ—ï¸  Architecture:")
        print("   Base: DictaBERT (dicta-il/dictabert-large-char-menaked)")
        print("   - Character-level BERT pretrained on Hebrew")
        print("   - Hidden size: 1024")
        print("   - Max length: 512 characters")
        
        print("\nğŸ“Š Custom Phoneme Head (4 classifiers):")
        print(f"   1. Consonant: Linear(1024 â†’ {len(CONSONANT_CLASSES)}) classes")
        print(f"      Classes: {CONSONANT_CLASSES[:5]}... + Ã˜")
        print(f"   2. Vowel: Linear(1024 â†’ {len(VOWEL_CLASSES)}) classes")
        print(f"      Classes: {VOWEL_CLASSES}")
        print(f"   3. Stress: Linear(1024 â†’ 2) [binary]")
        print(f"      Classes: [no-stress, stress-Ëˆ]")
        print(f"   4. Flip Vowel: Linear(1024 â†’ 2) [binary]")
        print(f"      Classes: [consonant-first, vowel-first]")
        
        print("\nğŸ“‰ Loss Function:")
        print("   Combined CrossEntropyLoss:")
        print("   loss = consonant_loss + vowel_loss + stress_loss + flip_loss")
        print("   - Ignores -100 labels (non-Hebrew chars)")
        
        print("\nğŸ“ Training:")
        print("   - Input: Full sentences (context-aware)")
        print("   - Labels: Per-character (4 labels per char)")
        print("   - Batch processing via HuggingFace Trainer")
        print("   - Option: freeze BERT / full fine-tuning")
        
        print("\nğŸ“ˆ Evaluation Metrics:")
        print("   - Per-head accuracy (consonant, vowel, stress, flip)")
        print("   - Overall accuracy (all 4 heads correct)")
        print("   - Character-level evaluation")
        
        print("\n" + "="*70)
    
    def test_data_format_example(self):
        """
        Show the expected data format for training.
        """
        print("\n" + "="*70)
        print("DATA FORMAT")
        print("="*70)
        
        print("\nğŸ“ Training Data Format (TSV):")
        print("   word<TAB>ipa")
        print("   ×©×œ×•×<TAB>Êƒa lËˆo Ã˜ m")
        print("   ×¢×¨×‘<TAB>Ê”Ëˆe re v")
        
        print("\nğŸ” IPA Format Requirements:")
        print("   1. Space-separated (one phoneme group per character)")
        print("   2. Normalized (râ†’Ê, gâ†’É¡, xâ†’Ï‡)")
        print("   3. Aligned (text length = IPA parts length)")
        print("   4. Silent chars use 'Ã˜'")
        
        print("\nğŸ’¾ Example Processing:")
        text = '×©×œ×•×'
        ipa = 'Êƒa lËˆo Ã˜ m'
        print(f"   Text: {text}")
        print(f"   IPA:  {ipa}")
        print(f"   Chars: {len(text)} = IPA parts: {len(ipa.split(' '))}")
        
        encoded = encode(text, ipa)
        print(f"\n   Encoded shapes:")
        print(f"   - consonant: {len(encoded.consonant)} labels")
        print(f"   - vowel:     {len(encoded.vowel)} labels")
        print(f"   - stress:    {len(encoded.stress)} labels")
        print(f"   - flip:      {len(encoded.flip_vowel)} labels")
        
        print("\n" + "="*70)


def test_quick_demo():
    """Quick one-liner demo for presentations"""
    print("\n" + "="*70)
    print("ğŸš€ QUICK DEMO")
    print("="*70)
    
    from src.tokenize import encode, decode, Prediction, reconstruct_sentence
    
    # Hebrew text
    text = '×©×œ×•× ×¢×•×œ×'
    
    # Training data (aligned IPA)
    ipa = 'Êƒa lËˆo Ã˜ m Ã˜ Ê”o lËˆa Ã˜ m'
    
    # Encode â†’ Predict â†’ Decode â†’ Reconstruct
    encoded = encode(text, ipa)
    preds = Prediction(encoded.consonant, encoded.vowel, encoded.stress, encoded.flip_vowel)
    decoded = decode(text, preds, preserve_unknown=True)
    final = reconstruct_sentence(text, decoded)
    
    print(f"\nInput:  {text}")
    print(f"Output: {final}")
    print("\nâœ¨ Hebrew â†’ IPA Phonemes")
    
    print("="*70 + "\n")
    
    assert final == 'ÊƒalËˆom Ê”olËˆam'


if __name__ == '__main__':
    # Run all demos
    pytest.main([__file__, '-v', '-s'])

